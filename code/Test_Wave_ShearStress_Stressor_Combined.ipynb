{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.tri import LinearTriInterpolator, TriAnalyzer, Triangulation\n",
    "from scipy.interpolate import griddata\n",
    "from netCDF4 import Dataset\n",
    "from osgeo import gdal, osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critical_shear_stress(D_meters, rhow=1024, nu=1e-6, s=2.65, g=9.81):\n",
    "    # D_meters = grain size in meters, can be array\n",
    "    # rhow = density of water in kg/m3\n",
    "    # nu = kinematic viscosity of water\n",
    "    # s = specific gravity of sediment\n",
    "    # g = acceleratin due to gravity\n",
    "    Dstar = ((g * (s-1))/ nu**2)**(1/3) * D_meters\n",
    "    SHcr = (0.3/(1+1.2*Dstar)) + 0.055*(1-np.exp(-0.02 * Dstar))\n",
    "    taucrit = rhow * (s - 1) * g * D_meters * SHcr #in Pascals\n",
    "    return taucrit\n",
    "\n",
    "def calculate_receptor_change_percentage(receptor_filename, data_diff, ofpath):\n",
    "    #gdal version\n",
    "    data_diff = np.transpose(data_diff)\n",
    "    data = gdal.Open(receptor_filename)\n",
    "    img = data.GetRasterBand(1)\n",
    "    receptor_array = img.ReadAsArray()\n",
    "    # transpose to be in same orientation as NetCDF\n",
    "    receptor_array[receptor_array < 0] = np.nan\n",
    "\n",
    "    pct_mobility = {'Receptor_Value': [],\n",
    "                        'Increased Deposition': [],\n",
    "                        'Reduced Deposition': [],\n",
    "                        'Reduced Erosion': [],\n",
    "                        'Increased Erosion': [],\n",
    "                        'No Change':[]}\n",
    "    \n",
    "    for unique_val in [i for i in np.unique(receptor_array) if ~np.isnan(i)]:\n",
    "        mask = receptor_array==unique_val\n",
    "        data_at_val = np.where(mask, data_diff, np.nan)\n",
    "        data_at_val = data_at_val.flatten()\n",
    "        data_at_val = data_at_val[~np.isnan(data_at_val)]\n",
    "        ncells = data_at_val.size\n",
    "        pct_mobility['Receptor_Value'].append(unique_val)\n",
    "        pct_mobility['Increased Deposition'].append(100 * np.size(np.flatnonzero(data_at_val==-2))/ncells)\n",
    "        pct_mobility['Reduced Deposition'].append(100 * np.size(np.flatnonzero(data_at_val>-1))/ncells)\n",
    "        pct_mobility['Reduced Erosion'].append(100 * np.size(np.flatnonzero(data_at_val==1))/ncells)\n",
    "        pct_mobility['Increased Erosion'].append(100 * np.size(np.flatnonzero(data_at_val==2))/ncells)\n",
    "        pct_mobility['No Change'].append(100 * np.size(np.flatnonzero(data_at_val==0))/ncells)\n",
    "\n",
    "        # print(f\" Receptor Value = {unique_val}um | decrease = {pct_decrease}% | increase = {pct_increase}% | no change = {pct_nochange}%\")\n",
    "    DF = pd.DataFrame(pct_mobility)\n",
    "    DF = DF.set_index('Receptor_Value')\n",
    "    DF.to_csv(os.path.join(ofpath, 'receptor_percent_change.csv'))\n",
    "\n",
    "def estimate_grid_spacing(x,y, nsamples=100):\n",
    "    import random\n",
    "    import sys\n",
    "    coords = list(set(zip(x,y)))\n",
    "    if nsamples != len(x):\n",
    "        points = [random.choice(coords) for i in range(nsamples)] # pick N random points\n",
    "    else:\n",
    "        points = coords\n",
    "    MD = []\n",
    "    for p0x, p0y in points:\n",
    "        minimum_distance = sys.maxsize\n",
    "        for px, py in coords:\n",
    "            distance = np.sqrt((p0x - px) ** 2 + (p0y - py) ** 2)\n",
    "            if (distance < minimum_distance) & (distance !=0):\n",
    "                minimum_distance = distance\n",
    "        MD.append(minimum_distance)\n",
    "    dxdy = np.median(MD)\n",
    "    return dxdy\n",
    "\n",
    "\n",
    "def calc_receptor_taucrit(receptor_filename, x, y, latlon=False):\n",
    "    if ((receptor_filename is not None) or (not receptor_filename == \"\")):\n",
    "        data = gdal.Open(receptor_filename)\n",
    "        img = data.GetRasterBand(1)\n",
    "        receptor_array = img.ReadAsArray()\n",
    "        receptor_array[receptor_array < 0] = 0\n",
    "        (upper_left_x, x_size, x_rotation, upper_left_y, y_rotation, y_size) = data.GetGeoTransform()\n",
    "        cols = data.RasterXSize\n",
    "        rows = data.RasterYSize\n",
    "        r_rows = np.arange(rows) * y_size + upper_left_y + (y_size / 2)\n",
    "        r_cols = np.arange(cols) * x_size + upper_left_x + (x_size / 2)\n",
    "        if latlon==True:\n",
    "            r_cols = np.where(r_cols<0, r_cols+360, r_cols)\n",
    "        x_grid, y_grid = np.meshgrid(r_cols, r_rows)\n",
    "        receptor_array = griddata((x_grid.flatten(), y_grid.flatten()), receptor_array.flatten(), (x,y), method='nearest', fill_value=0)\n",
    "\n",
    "        taucrit = critical_shear_stress(D_meters=receptor_array * 1e-6,\n",
    "                            rhow=1024,\n",
    "                            nu=1e-6,\n",
    "                            s=2.65,\n",
    "                            g=9.81) #units N/m2 = Pa\n",
    "    else:\n",
    "        # taucrit without a receptor\n",
    "        #Assume the following grain sizes and conditions for typical beach sand (Nielsen, 1992 p.108)\n",
    "        taucrit = critical_shear_stress(D_meters=200*1e-6, rhow=1024, nu=1e-6, s=2.65, g=9.81)  #units N/m2 = Pa\n",
    "    return taucrit, receptor_array\n",
    "\n",
    "def create_structured_array_from_unstructured(x, y, z, dxdy, flatness=0.2):\n",
    "    # flatness is from 0-.5 .5 is equilateral triangle\n",
    "    refx = np.arange(np.nanmin(x), np.nanmax(x)+dxdy, dxdy)\n",
    "    refy = np.arange(np.nanmin(y), np.nanmax(y)+dxdy, dxdy)\n",
    "    refxg, refyg = np.meshgrid(refx, refy)\n",
    "    tri = Triangulation(x, y)\n",
    "    mask = TriAnalyzer(tri).get_flat_tri_mask(flatness)\n",
    "    tri.set_mask(mask)\n",
    "    tli = LinearTriInterpolator(tri, z)\n",
    "    z_interp = tli(refxg, refyg)\n",
    "    return refxg, refyg, z_interp.data\n",
    "\n",
    "def classify_mobility(mobility_parameter_dev, mobility_parameter_nodev):\n",
    "    mobility_classification = np.zeros(mobility_parameter_dev.shape)\n",
    "    #Reduced Erosion (Tw<Tb) & (Tw-Tb)>1\n",
    "    mobility_classification = np.where(((mobility_parameter_dev < mobility_parameter_nodev) & (mobility_parameter_nodev>=1)), 1, mobility_classification)\n",
    "    #Increased Erosion (Tw>Tb) & (Tw-Tb)>1\n",
    "    mobility_classification = np.where(((mobility_parameter_dev > mobility_parameter_nodev) & (mobility_parameter_nodev>=1)), 2, mobility_classification)\n",
    "    #Reduced Deposition (Tw>Tb) & (Tw-Tb)<1\n",
    "    mobility_classification = np.where(((mobility_parameter_dev > mobility_parameter_nodev) & (mobility_parameter_nodev<1)), -1, mobility_classification)\n",
    "    #Increased Deposition (Tw>Tb) & (Tw-Tb)>1\n",
    "    mobility_classification = np.where(((mobility_parameter_dev < mobility_parameter_nodev) & (mobility_parameter_nodev<1)), -2, mobility_classification)\n",
    "    #NoChange = 0\n",
    "    return mobility_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_nodev = r\"H:\\Projects\\C1308_SEAT\\SEAT_inputs\\plugin-input\\oregon\\devices-not-present\"\n",
    "fpath_dev = r\"H:\\Projects\\C1308_SEAT\\SEAT_inputs\\plugin-input\\oregon\\devices-present\"\n",
    "probabilities_file = r\"H:\\Projects\\C1308_SEAT\\SEAT_inputs\\plugin-input\\oregon\\boundary-condition\\BC_Annie_Annual_SETS.csv\"\n",
    "latlon=True\n",
    "receptor_filename = r\"H:\\Projects\\C1308_SEAT\\SEAT_inputs\\plugin-input\\oregon\\receptor\\grainsize_receptor.tif\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'This_is_a_test_02_map.nc'\n",
    "int(file.split('.')[0].split('_')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dev_present = Dataset(os.path.join(fpath_nodev, files_nodev[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grid_define_vars(dataset):\n",
    "    vars = list(file_dev_present.variables)\n",
    "    if 'XCOR' in vars:\n",
    "        gridtype = 'structured'\n",
    "        xvar = 'XCOR'\n",
    "        yvar = 'YCOR'\n",
    "        tauvar = 'TAUMAX'\n",
    "    else:\n",
    "        gridtype = 'unstructured'\n",
    "        xvar = 'FlowElem_xcc'\n",
    "        yvar = 'FlowElem_ycc'\n",
    "        tauvar = 'taus'\n",
    "    return gridtype, xvar, yvar, tauvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('structured', 'XCOR', 'YCOR', 'TAUMAX')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_grid_define_vars(file_dev_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_nodev = [i for i in os.listdir(fpath_nodev) if i.endswith('.nc')]\n",
    "files_dev = [i for i in os.listdir(fpath_dev) if i.endswith('.nc')]\n",
    "\n",
    "# Load and sort files\n",
    "if len(files_nodev) == 1 & len(files_dev) ==1: \n",
    "    #asumes a concatonated files with shape\n",
    "    #[run_order, time, rows, cols]\n",
    "\n",
    "    file_dev_present = Dataset(os.path.join(fpath_nodev, files_nodev[0]))\n",
    "    gridtype, xvar, yvar, tauvar = check_grid_define_vars(file_dev_present)\n",
    "    # X-coordinate of cell center\n",
    "    xcor = file_dev_present.variables[xvar][:].data\n",
    "    # Y-coordinate of cell center\n",
    "    ycor = file_dev_present.variables[yvar][:].data\n",
    "    # TAUMAX\n",
    "    tau_dev = file_dev_present.variables[tauvar][:]\n",
    "    # close the device prsent file\n",
    "    file_dev_present.close()\n",
    "\n",
    "    file_dev_notpresent = Dataset(os.path.join(fpath_dev, files_dev[0]))\n",
    "    tau_nodev = file_dev_notpresent.variables[tauvar][:]\n",
    "    # close the device not present file\n",
    "    file_dev_notpresent.close()\n",
    "\n",
    "else: \n",
    "    #asumes each run is separate with the some_name_RunNum_map.nc, where run number comes at the last underscore before _map.nc\n",
    "    runorder_nodev = np.zeros((len(files_nodev)))\n",
    "    for ic, file in enumerate(files_nodev):\n",
    "        runorder_nodev[ic] = int(file.split('.')[0].split('_')[-2])\n",
    "    runorder_dev = np.zeros((len(files_dev)))\n",
    "    for ic, file in enumerate(files_dev):\n",
    "        runorder_dev[ic] = int(file.split('.')[0].split('_')[-2])\n",
    "\n",
    "    #ensure run oder for nodev matches dev files\n",
    "    if np.any(runorder_nodev != runorder_dev):\n",
    "        adjust_dev_order = []\n",
    "        for ri in runorder_nodev:\n",
    "            adjust_dev_order = np.append(adjust_dev_order, np.flatnonzero(runorder_dev == ri))\n",
    "        files_dev = [files_dev[int(i)] for i in adjust_dev_order]\n",
    "        runorder_dev = [runorder_dev[int(i)] for i in adjust_dev_order]\n",
    "    DF = pd.DataFrame({'files_nodev':files_nodev, \n",
    "                'run_order_nodev':runorder_nodev,\n",
    "                'files_dev':files_dev,\n",
    "                'run_order_dev':runorder_dev})\n",
    "    DF = DF.sort_values(by=runorder_nodev)\n",
    "\n",
    "    first_run = True\n",
    "    ir = 0\n",
    "    for _, row in DF.iterrows():\n",
    "        file_dev_notpresent = Dataset(os.path.join(fpath_nodev, row.files_nodev))\n",
    "        file_dev_present = Dataset(os.path.join(fpath_dev, row.files_dev))\n",
    "\n",
    "        gridtype, xvar, yvar, tauvar = check_grid_define_vars(file_dev_present)\n",
    "\n",
    "        if first_run:\n",
    "            tmp = file_dev_notpresent.variables[tauvar].data\n",
    "            tau_nodev = np.zeros(DF.shape[0], tmp.shape[0], tmp.shape[1], tmp.shape[2])\n",
    "            tau_dev = np.zeros(DF.shape[0], tmp.shape[0], tmp.shape[1], tmp.shape[2])\n",
    "            xcor = file_dev_notpresent.variables[xvar][:].data\n",
    "            ycor = file_dev_notpresent.variables[yvar][:].data            \n",
    "            first_run = False\n",
    "        tau_nodev = file_dev_notpresent.variables[tauvar].data\n",
    "        tau_dev = file_dev_present.variables[tauvar].data\n",
    "        ir += 1\n",
    "# Finished loading and sorting files\n",
    "\n",
    "# Load BC file with probabilities and find appropriate probability\n",
    "BC_probability = pd.read_csv(probabilities_file, delimiter=\",\")\n",
    "BC_probability['run order'] = BC_probability['run order']-1\n",
    "BC_probability = BC_probability.sort_values(by='run order')\n",
    "BC_probability\n",
    "\n",
    "# Calculate Stressor and Receptors\n",
    "# data_dev_max = np.amax(data_dev, axis=1, keepdims=True) #look at maximum shear stress difference change\n",
    "tau_dev_max = np.amax(tau_dev, axis=1, keepdims=True) #max over time\n",
    "tau_nodev_max = np.amax(tau_dev, axis=1, keepdims=True) #max over time\n",
    "\n",
    "#initialize arrays\n",
    "taumax_combined_nodev = np.zeros(np.shape(tau_nodev[0, 0, :, :]))\n",
    "taumax_combined_dev = np.zeros(np.shape(tau_dev[0, 0, :, :]))\n",
    "\n",
    "for run_number, prob in zip(BC_probability['run order'].values,\n",
    "                             BC_probability[\"% of yr\"].values):\n",
    "        \n",
    "    taumax_combined_nodev = taumax_combined_nodev + prob * tau_nodev_max[run_number,-1,:,:] #tau_max #from last model run\n",
    "    taumax_combined_dev = taumax_combined_dev + prob * tau_dev_max[run_number,-1,:,:] #tau_max #from maximum of timeseries\n",
    "\n",
    "tau_diff = taumax_combined_dev - taumax_combined_nodev\n",
    "taucrit, receptor_array = calc_receptor_taucrit(receptor_filename, xcor, ycor, latlon=True)\n",
    "mobility_parameter_nodev = taumax_combined_nodev / taucrit\n",
    "mobility_parameter_nodev = np.where(receptor_array==0, 0, mobility_parameter_nodev)\n",
    "mobility_parameter_dev = taumax_combined_dev / taucrit\n",
    "mobility_parameter_dev = np.where(receptor_array==0, 0, mobility_parameter_dev)\n",
    "# Calculate risk metrics over all runs\n",
    "\n",
    "mobility_parameter_diff = mobility_parameter_dev - mobility_parameter_nodev\n",
    "\n",
    "if gridtype=='structured':\n",
    "    mobility_classification = classify_mobility(mobility_parameter_dev, mobility_parameter_nodev)\n",
    "    listOfFiles = [tau_diff, mobility_parameter_nodev, mobility_parameter_dev, mobility_parameter_diff, mobility_classification]\n",
    "    dx = np.nanmean(np.diff(xcor[:,0]))\n",
    "    dy = np.nanmean(np.diff(ycor[0,:]))\n",
    "    rx = xcor\n",
    "    ry = ycor\n",
    "else: # unstructured\n",
    "    dxdy = estimate_grid_spacing(xcor,ycor, nsamples=100)\n",
    "    rx, ry, tau_diff_struct = create_structured_array_from_unstructured(xcor, ycor, tau_diff, dxdy, flatness=0.2)\n",
    "    _, _, mobility_parameter_nodev_struct = create_structured_array_from_unstructured(xcor, ycor, mobility_parameter_nodev, dxdy, flatness=0.2)\n",
    "    _, _, mobility_parameter_dev_struct = create_structured_array_from_unstructured(xcor, ycor, mobility_parameter_dev, dxdy, flatness=0.2)\n",
    "    _, _, mobility_parameter_diff_struct = create_structured_array_from_unstructured(xcor, ycor, mobility_parameter_diff, dxdy, flatness=0.2)\n",
    "    mobility_classification = classify_mobility(mobility_parameter_dev_struct, mobility_parameter_nodev_struct)\n",
    "    listOfFiles = [tau_diff_struct, mobility_parameter_nodev_struct, mobility_parameter_dev_struct, mobility_parameter_diff_struct, mobility_classification]\n",
    "\n",
    "# return listOfFiles, rx, ry, dxdy, dxdy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_dev_max = np.amax(data_dev, axis=1, keepdims=True) #look at maximum shear stress difference change\n",
    "tau_dev_max = np.amax(tau_dev, axis=1, keepdims=True) #max over time\n",
    "tau_nodev_max = np.amax(tau_dev, axis=1, keepdims=True) #max over time\n",
    "\n",
    "#initialize arrays\n",
    "taumax_combined_nodev = np.zeros(np.shape(tau_nodev[0, 0, :, :]))\n",
    "taumax_combined_dev = np.zeros(np.shape(tau_dev[0, 0, :, :]))\n",
    "\n",
    "for run_number, prob in zip(BC_probability['run order'].values,\n",
    "                             BC_probability[\"% of yr\"].values):\n",
    "        \n",
    "    taumax_combined_nodev = taumax_combined_nodev + prob * tau_nodev_max[run_number,-1,:,:] #tau_max #from last model run\n",
    "    taumax_combined_dev = taumax_combined_dev + prob * tau_dev_max[run_number,-1,:,:] #tau_max #from maximum of timeseries\n",
    "\n",
    "tau_diff = taumax_combined_dev - taumax_combined_nodev\n",
    "taucrit, receptor_array = calc_receptor_taucrit(receptor_filename, xcor, ycor, latlon=True)\n",
    "mobility_parameter_nodev = taumax_combined_nodev / taucrit\n",
    "mobility_parameter_nodev = np.where(receptor_array==0, 0, mobility_parameter_nodev)\n",
    "mobility_parameter_dev = taumax_combined_dev / taucrit\n",
    "mobility_parameter_dev = np.where(receptor_array==0, 0, mobility_parameter_dev)\n",
    "# Calculate risk metrics over all runs\n",
    "\n",
    "mobility_parameter_diff = mobility_parameter_dev - mobility_parameter_nodev\n",
    "\n",
    "if gridtype=='structured':\n",
    "    mobility_classification = classify_mobility(mobility_parameter_dev, mobility_parameter_nodev)\n",
    "    listOfFiles = [tau_diff, mobility_parameter_nodev, mobility_parameter_dev, mobility_parameter_diff, mobility_classification]\n",
    "    dx = np.nanmean(np.diff(xcor[:,0]))\n",
    "    dy = np.nanmean(np.diff(ycor[0,:]))\n",
    "    rx = xcor\n",
    "    ry = ycor\n",
    "else: # unstructured\n",
    "    dxdy = estimate_grid_spacing(xcor,ycor, nsamples=100)\n",
    "    rx, ry, tau_diff_struct = create_structured_array_from_unstructured(xcor, ycor, tau_diff, dxdy, flatness=0.2)\n",
    "    _, _, mobility_parameter_nodev_struct = create_structured_array_from_unstructured(xcor, ycor, mobility_parameter_nodev, dxdy, flatness=0.2)\n",
    "    _, _, mobility_parameter_dev_struct = create_structured_array_from_unstructured(xcor, ycor, mobility_parameter_dev, dxdy, flatness=0.2)\n",
    "    _, _, mobility_parameter_diff_struct = create_structured_array_from_unstructured(xcor, ycor, mobility_parameter_diff, dxdy, flatness=0.2)\n",
    "    mobility_classification = classify_mobility(mobility_parameter_dev_struct, mobility_parameter_nodev_struct)\n",
    "    listOfFiles = [tau_diff_struct, mobility_parameter_nodev_struct, mobility_parameter_dev_struct, mobility_parameter_diff_struct, mobility_classification]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C3507_SAM_HAB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
